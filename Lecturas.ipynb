{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reporte: Attention Is All You Need**  \n",
    "\n",
    "### Resumen del artículo  \n",
    "El artículo *Attention Is All You Need* introduce un modelo que transformó por completo el panorama del procesamiento de lenguaje natural (NLP) y, en general, de muchas áreas relacionadas con aprendizaje profundo. Los autores presentan un enfoque basado exclusivamente en un mecanismo de atención, prescindiendo de las redes recurrentes (RNNs) y convolucionales (CNNs) que, hasta ese momento, eran la norma para procesar datos secuenciales como texto o audio.  \n",
    "\n",
    "El Transformer, como llamaron a este modelo, está diseñado en dos partes principales: un **codificador** que analiza y abstrae la información de entrada y un **decodificador** que genera la salida deseada. Cada uno de estos componentes está compuesto por múltiples capas idénticas formadas por dos submódulos clave:  \n",
    "1. **Mecanismo de autoatención multi-cabeza (multi-head self-attention)**: Este submódulo permite que el modelo se enfoque simultáneamente en diferentes aspectos del contexto global de una secuencia, asignando un peso diferente a cada palabra según su importancia relativa en la oración.  \n",
    "2. **Redes completamente conectadas (feed-forward networks)**: Estas redes aplican transformaciones no lineales para refinar las representaciones obtenidas en el submódulo anterior.  \n",
    "\n",
    "Otro detalle importante es la incorporación de **embeddings posicionales**. Dado que el Transformer no procesa los datos de forma secuencial, como las RNNs, necesita información adicional para captar el orden de las palabras. Los embeddings posicionales codifican esta información en vectores que se combinan con los embeddings de las palabras antes de entrar al modelo.  \n",
    "\n",
    "El enfoque paralelo del Transformer no solo mejora la precisión en tareas como la traducción automática, sino que también reduce significativamente el tiempo de entrenamiento, superando a las RNNs y CNNs en eficiencia computacional. Gracias a esta estructura, el Transformer se convirtió en la base de modelos posteriores como **BERT**, **GPT** y **T5**, que dominan hoy en día el campo de NLP.  \n",
    "\n",
    "### ¿Qué me pareció lo más interesante?  \n",
    "Lo que más me impactó fue la eliminación de las RNNs. Antes de este artículo, se pensaba que las redes recurrentes eran imprescindibles para tareas secuenciales porque su arquitectura permite analizar datos en orden, manteniendo un \"contexto\" mediante estados ocultos. Sin embargo, los autores demostraron que el mecanismo de atención podía manejar estas relaciones de manera mucho más eficiente. Esto es un cambio de paradigma que, honestamente, nunca hubiera imaginado posible.  \n",
    "\n",
    "Otra cosa que me fascinó es cómo el mecanismo de autoatención asigna pesos a cada palabra para determinar su relevancia en relación con otras palabras en la oración. Por ejemplo, en una frase como \"El perro que corre en el parque es mío\", el modelo puede identificar que \"perro\" y \"mío\" están más relacionados, ignorando las palabras menos importantes en ese contexto. Este tipo de análisis se hace en paralelo, lo que hace al Transformer mucho más rápido y preciso que las RNNs, que procesan palabra por palabra.  \n",
    "\n",
    "Finalmente, me pareció increíble cómo las *attention heads* pueden dividirse el trabajo para analizar distintas partes de una oración. Mientras una cabeza podría enfocarse en relaciones gramaticales, otra podría estar evaluando el contexto semántico. Esta capacidad de análisis simultáneo es lo que da al Transformer su ventaja frente a otras arquitecturas.  \n",
    "\n",
    "### Aspectos que no comprendí completamente  \n",
    "A pesar de haber leído varias veces la sección técnica, todavía no entiendo del todo cómo funcionan las operaciones matemáticas detrás del mecanismo de autoatención. Sé que se utilizan tres matrices clave: **Q (queries)**, **K (keys)** y **V (values)**, y que estas se derivan de los embeddings de las palabras. Estas matrices luego se combinan para calcular un puntaje de atención (un escalar) que determina qué tan importante es cada palabra respecto a las demás. Sin embargo, no me queda claro cómo se normalizan estos puntajes con *softmax* y cómo afectan los resultados finales.  \n",
    "\n",
    "Otro punto que me generó dudas es la implementación de los embeddings posicionales. Entiendo que se suman a los embeddings de las palabras, pero no estoy seguro de cómo se diseñan estos vectores posicionales. ¿Se generan de forma estática o se aprenden durante el entrenamiento? Además, me pregunto cómo afecta esta información posicional en tareas donde el orden no es tan importante.  \n",
    "\n",
    "Por último, aunque el artículo menciona que el procesamiento paralelo mejora la eficiencia computacional, me pregunto si no existe alguna pérdida de información al no procesar las palabras de manera secuencial. Es decir, ¿cómo se asegura el modelo de que las relaciones temporales entre palabras no se diluyan cuando las analiza en paralelo?  \n",
    "\n",
    "### Principales aplicaciones de estos modelos  \n",
    "Desde su publicación, el Transformer ha transformado por completo el campo del NLP y ha dado lugar a modelos aún más sofisticados. Algunos ejemplos destacados de sus aplicaciones son:  \n",
    "1. **Traducción automática**: Herramientas como Google Translate han integrado modelos basados en Transformers para mejorar la fluidez y precisión en la traducción entre idiomas. Por ejemplo, ahora pueden captar mejor el contexto cultural de ciertas expresiones.  \n",
    "2. **Análisis de sentimientos**: En redes sociales, empresas utilizan estos modelos para evaluar cómo los usuarios perciben sus productos o servicios. Los Transformers permiten identificar no solo palabras clave, sino también el tono emocional de los mensajes.  \n",
    "3. **Asistentes virtuales**: Tecnologías como Alexa, Siri y Google Assistant han mejorado gracias a los Transformers, especialmente en la comprensión de preguntas complejas o solicitudes ambiguas.  \n",
    "4. **Generación de texto**: Modelos como GPT-3 son capaces de escribir artículos, responder preguntas, e incluso programar código, demostrando la versatilidad de esta arquitectura.  \n",
    "\n",
    "Además, los Transformers han comenzado a utilizarse en áreas más allá del NLP, como en la visión por computadora y el análisis de series temporales, lo que demuestra su flexibilidad.  \n",
    "\n",
    "### ¿Se me ocurre alguna aplicación novedosa?  \n",
    "Una posible aplicación sería utilizar Transformers en **biología computacional**, específicamente para analizar secuencias de ADN o ARN. En este caso, los \"tokens\" serían los codones, y el modelo podría identificar patrones importantes relacionados con mutaciones genéticas o enfermedades hereditarias. La capacidad del Transformer para procesar grandes cantidades de datos secuenciales sería ideal para este tipo de análisis.  \n",
    "\n",
    "Otra idea sería integrarlos en la **educación personalizada**. Un chatbot basado en Transformers podría adaptarse al estilo de aprendizaje de cada estudiante, ajustando el nivel de dificultad de los contenidos o sugiriendo actividades basadas en las respuestas previas del usuario. Esto podría revolucionar el acceso a la educación de calidad.  \n",
    "\n",
    "Finalmente, en el ámbito financiero, podría usarse para analizar noticias, redes sociales y datos históricos al mismo tiempo, prediciendo el comportamiento del mercado con un enfoque más contextualizado. Este tipo de herramienta sería valiosa para los inversionistas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reporte: BERT - Bidirectional Encoder Representations from Transformers**\n",
    "\n",
    "### Resumen del artículo  \n",
    "El artículo que introduce a **BERT** representa uno de los avances más significativos en el procesamiento de lenguaje natural (NLP) en los últimos años. Los autores presentan una arquitectura basada en Transformers, específicamente en el codificador, pero con un diseño innovador que permite al modelo entender el contexto de una palabra en ambas direcciones (izquierda y derecha) dentro de una oración. Esto es lo que se conoce como un modelo bidireccional.  \n",
    "\n",
    "Una de las contribuciones clave de BERT es el enfoque de **entrenamiento previo y ajuste fino** (*pre-training and fine-tuning*). Primero, el modelo se preentrena en tareas generales de lenguaje como:  \n",
    "1. **Predicción de palabras enmascaradas (Masked Language Model o MLM)**: Durante el preentrenamiento, el modelo enmascara aleatoriamente algunas palabras en las oraciones y aprende a predecirlas basándose en el contexto restante. Por ejemplo, en \"El [MASK] corre rápido\", el modelo debe predecir que la palabra faltante es \"perro\".  \n",
    "2. **Predicción de la próxima oración (Next Sentence Prediction o NSP)**: Aquí, el modelo aprende a identificar si dos oraciones están conectadas lógicamente. Esto es útil para tareas como responder preguntas o análisis de párrafos largos.  \n",
    "\n",
    "Una vez que BERT ha aprendido estas representaciones generales, se ajusta específicamente para tareas de NLP como clasificación de texto, análisis de sentimientos, preguntas y respuestas, entre otras.  \n",
    "\n",
    "La principal ventaja de BERT frente a modelos anteriores es su capacidad para entender relaciones complejas dentro de una oración y entre oraciones. Al utilizar representaciones contextuales bidireccionales, supera limitaciones de modelos unidireccionales como **GPT**, que solo procesan el texto en una dirección.  \n",
    "\n",
    "### ¿Qué me pareció lo más interesante?  \n",
    "Lo que más me llamó la atención es cómo BERT lleva el concepto de *transfer learning* a un nivel completamente nuevo. Antes, entrenar modelos desde cero era común, pero extremadamente costoso en términos de recursos y tiempo. Con BERT, los investigadores demostraron que es posible aprovechar un modelo preentrenado en tareas generales y ajustarlo para necesidades específicas con datos limitados. Esto no solo hace que el desarrollo sea más eficiente, sino que también democratiza el acceso a tecnologías avanzadas para instituciones con menos recursos.  \n",
    "\n",
    "Otra cosa que me fascinó es el mecanismo de **MLM**. Los modelos tradicionales tienden a procesar texto de izquierda a derecha (o viceversa), lo que significa que solo tienen en cuenta una parte del contexto en cada paso. Pero BERT rompe esa barrera al analizar todo el contexto simultáneamente. Por ejemplo, en una frase ambigua como \"La caja fuerte está en el banco\", el modelo puede inferir si \"banco\" se refiere a una institución financiera o a un asiento dependiendo del resto de las palabras.  \n",
    "\n",
    "También me sorprendió cómo el modelo NSP ayuda a comprender relaciones entre oraciones. Aunque parece un detalle técnico, esto tiene aplicaciones prácticas enormes, como mejorar los sistemas de preguntas y respuestas o facilitar resúmenes automáticos más coherentes.  \n",
    "\n",
    "### Aspectos que no comprendí completamente  \n",
    "A pesar de su claridad, el artículo tiene partes técnicas que todavía me resultan complejas. Por ejemplo, el proceso exacto de entrenamiento con MLM es algo que entiendo a nivel conceptual, pero no en profundidad. Me pregunto cómo se seleccionan las palabras que serán enmascaradas y qué impacto tiene esta selección en el rendimiento del modelo. Además, no me queda claro cómo el modelo evita \"hacer trampa\" prediciendo las palabras en función de patrones repetidos en el dataset de entrenamiento.  \n",
    "\n",
    "Otro punto que no comprendí del todo es cómo funciona la interacción entre MLM y NSP durante el preentrenamiento. Aunque ambas tareas parecen complementarias, no estoy seguro de cómo el modelo equilibra el aprendizaje de estas dos habilidades al mismo tiempo. ¿Hay algún tipo de ponderación para decidir qué tarea es más importante en cada etapa?  \n",
    "\n",
    "Finalmente, algo que me dejó un poco confundido es cómo BERT maneja la longitud de las oraciones. Dado que el modelo tiene un límite de tokens (512), me pregunto cómo afecta esta restricción a textos largos o complejos. ¿Se pierde información importante? ¿Qué sucede con textos científicos o legales donde cada palabra puede ser crucial?  \n",
    "\n",
    "### Principales aplicaciones de estos modelos  \n",
    "Desde su aparición, BERT ha marcado un antes y un después en NLP, estableciendo nuevos estándares en muchas tareas. Algunas de sus aplicaciones más importantes son:  \n",
    "1. **Clasificación de texto**: BERT se utiliza para categorizar documentos, correos electrónicos, o publicaciones en redes sociales. Por ejemplo, puede clasificar automáticamente reseñas de productos como positivas o negativas.  \n",
    "2. **Sistemas de preguntas y respuestas**: Muchos motores de búsqueda, como Google, han integrado BERT para responder preguntas de manera más precisa. Esto ha mejorado significativamente la capacidad de los sistemas para entender preguntas complejas y contextos amplios.  \n",
    "3. **Traducción automática**: Aunque BERT no está diseñado específicamente para traducir, su capacidad de comprensión contextual lo hace útil en tareas relacionadas con la traducción o el análisis semántico entre idiomas.  \n",
    "4. **Asistentes virtuales**: BERT ha mejorado el entendimiento de comandos y solicitudes en asistentes como Siri, Alexa, o Google Assistant, haciéndolos más efectivos y naturales.  \n",
    "5. **Detección de spam o fraude**: Empresas de tecnología y finanzas utilizan BERT para identificar patrones sospechosos en correos, transacciones o comentarios en línea.  \n",
    "\n",
    "Además, debido a su flexibilidad, BERT ha inspirado variantes como **RoBERTa**, **DistilBERT** y **ALBERT**, que optimizan su desempeño en distintos escenarios.  \n",
    "\n",
    "### ¿Se me ocurre alguna aplicación novedosa?  \n",
    "BERT podría tener un impacto significativo en el campo de la educación, específicamente en el diseño de sistemas de retroalimentación automática para estudiantes. Por ejemplo, podría analizar ensayos escritos y proporcionar comentarios personalizados sobre gramática, estilo y argumentos lógicos. Esto sería especialmente útil en programas de aprendizaje de idiomas.  \n",
    "\n",
    "Otra idea sería aplicarlo en **biología computacional**, adaptándolo para analizar relaciones entre genes o interacciones moleculares. Las secuencias genéticas podrían representarse como \"oraciones\" y los genes como \"palabras\", permitiendo que el modelo identifique patrones importantes para predecir enfermedades o diseñar medicamentos personalizados.  \n",
    "\n",
    "En el ámbito legal, BERT podría usarse para analizar contratos o documentos extensos, destacando cláusulas específicas según las necesidades del usuario. Esto facilitaría a abogados y empresas revisar documentos legales con mayor rapidez y precisión.  \n",
    "\n",
    "Finalmente, creo que también sería interesante explorar su uso en psicología, analizando grandes volúmenes de texto como diarios o publicaciones en redes sociales para identificar patrones relacionados con trastornos emocionales o de salud mental, ofreciendo una herramienta complementaria para el diagnóstico.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITESO_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
